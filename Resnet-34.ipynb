{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import activations\n",
    "import numpy as np\n",
    "\n",
    "import six\n",
    "from keras.models import Model\n",
    "from keras.layers import (\n",
    "    Input,\n",
    "    Activation,\n",
    "    Dense,\n",
    "    Flatten\n",
    ")\n",
    "from keras.layers.convolutional import (\n",
    "    Conv2D,\n",
    "    MaxPooling2D,\n",
    "    AveragePooling2D\n",
    ")\n",
    "from keras.layers.merge import add\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "def _bn_relu(input):\n",
    "    \"\"\"Helper to build a BN -> relu block\n",
    "    \"\"\"\n",
    "    norm = BatchNormalization(axis=CHANNEL_AXIS)(input)\n",
    "    return Activation(\"relu\")(norm)\n",
    "\n",
    "\n",
    "def _conv_bn_relu(**conv_params):\n",
    "    \"\"\"Helper to build a conv -> BN -> relu block\n",
    "    \"\"\"\n",
    "    filters = conv_params[\"filters\"]\n",
    "    kernel_size = conv_params[\"kernel_size\"]\n",
    "    strides = conv_params.setdefault(\"strides\", (1, 1))\n",
    "    kernel_initializer = conv_params.setdefault(\"kernel_initializer\", \"he_normal\")\n",
    "    padding = conv_params.setdefault(\"padding\", \"same\")\n",
    "    kernel_regularizer = conv_params.setdefault(\"kernel_regularizer\", l2(1.e-4))\n",
    "\n",
    "    def f(input):\n",
    "        conv = Conv2D(filters=filters, kernel_size=kernel_size,\n",
    "                      strides=strides, padding=padding,\n",
    "                      kernel_initializer=kernel_initializer,\n",
    "                      kernel_regularizer=kernel_regularizer)(input)\n",
    "        return _bn_relu(conv)\n",
    "\n",
    "    return f\n",
    "\n",
    "\n",
    "def _bn_relu_conv(**conv_params):\n",
    "    \"\"\"Helper to build a BN -> relu -> conv block.\n",
    "    This is an improved scheme proposed in http://arxiv.org/pdf/1603.05027v2.pdf\n",
    "    \"\"\"\n",
    "    filters = conv_params[\"filters\"]\n",
    "    kernel_size = conv_params[\"kernel_size\"]\n",
    "    strides = conv_params.setdefault(\"strides\", (1, 1))\n",
    "    kernel_initializer = conv_params.setdefault(\"kernel_initializer\", \"he_normal\")\n",
    "    padding = conv_params.setdefault(\"padding\", \"same\")\n",
    "    kernel_regularizer = conv_params.setdefault(\"kernel_regularizer\", l2(1.e-4))\n",
    "\n",
    "    def f(input):\n",
    "        activation = _bn_relu(input)\n",
    "        return Conv2D(filters=filters, kernel_size=kernel_size,\n",
    "                      strides=strides, padding=padding,\n",
    "                      kernel_initializer=kernel_initializer,\n",
    "                      kernel_regularizer=kernel_regularizer)(activation)\n",
    "\n",
    "    return f\n",
    "\n",
    "\n",
    "def _shortcut(input, residual):\n",
    "    \"\"\"Adds a shortcut between input and residual block and merges them with \"sum\"\n",
    "    \"\"\"\n",
    "    # Expand channels of shortcut to match residual.\n",
    "    # Stride appropriately to match residual (width, height)\n",
    "    # Should be int if network architecture is correctly configured.\n",
    "    input_shape = K.int_shape(input)\n",
    "    residual_shape = K.int_shape(residual)\n",
    "    stride_width = int(round(input_shape[ROW_AXIS] / residual_shape[ROW_AXIS]))\n",
    "    stride_height = int(round(input_shape[COL_AXIS] / residual_shape[COL_AXIS]))\n",
    "    equal_channels = input_shape[CHANNEL_AXIS] == residual_shape[CHANNEL_AXIS]\n",
    "\n",
    "    shortcut = input\n",
    "    # 1 X 1 conv if shape is different. Else identity.\n",
    "    if stride_width > 1 or stride_height > 1 or not equal_channels:\n",
    "        shortcut = Conv2D(filters=residual_shape[CHANNEL_AXIS],\n",
    "                          kernel_size=(1, 1),\n",
    "                          strides=(stride_width, stride_height),\n",
    "                          padding=\"valid\",\n",
    "                          kernel_initializer=\"he_normal\",\n",
    "                          kernel_regularizer=l2(0.0001))(input)\n",
    "\n",
    "    return add([shortcut, residual])\n",
    "\n",
    "\n",
    "def _residual_block(block_function, filters, repetitions, is_first_layer=False):\n",
    "    \"\"\"Builds a residual block with repeating bottleneck blocks.\n",
    "    \"\"\"\n",
    "    def f(input):\n",
    "        for i in range(repetitions):\n",
    "            init_strides = (1, 1)\n",
    "            if i == 0 and not is_first_layer:\n",
    "                init_strides = (2, 2)\n",
    "            input = block_function(filters=filters, init_strides=init_strides,\n",
    "                                   is_first_block_of_first_layer=(is_first_layer and i == 0))(input)\n",
    "        return input\n",
    "\n",
    "    return f\n",
    "\n",
    "\n",
    "def basic_block(filters, init_strides=(1, 1), is_first_block_of_first_layer=False):\n",
    "    \"\"\"Basic 3 X 3 convolution blocks for use on resnets with layers <= 34.\n",
    "    Follows improved proposed scheme in http://arxiv.org/pdf/1603.05027v2.pdf\n",
    "    \"\"\"\n",
    "    def f(input):\n",
    "\n",
    "        if is_first_block_of_first_layer:\n",
    "            # don't repeat bn->relu since we just did bn->relu->maxpool\n",
    "            conv1 = Conv2D(filters=filters, kernel_size=(3, 3),\n",
    "                           strides=init_strides,\n",
    "                           padding=\"same\",\n",
    "                           kernel_initializer=\"he_normal\",\n",
    "                           kernel_regularizer=l2(1e-4))(input)\n",
    "        else:\n",
    "            conv1 = _bn_relu_conv(filters=filters, kernel_size=(3, 3),\n",
    "                                  strides=init_strides)(input)\n",
    "\n",
    "        residual = _bn_relu_conv(filters=filters, kernel_size=(3, 3))(conv1)\n",
    "        return _shortcut(input, residual)\n",
    "\n",
    "    return f\n",
    "\n",
    "\n",
    "def bottleneck(filters, init_strides=(1, 1), is_first_block_of_first_layer=False):\n",
    "    \"\"\"Bottleneck architecture for > 34 layer resnet.\n",
    "    Follows improved proposed scheme in http://arxiv.org/pdf/1603.05027v2.pdf\n",
    "    Returns:\n",
    "        A final conv layer of filters * 4\n",
    "    \"\"\"\n",
    "    def f(input):\n",
    "\n",
    "        if is_first_block_of_first_layer:\n",
    "            # don't repeat bn->relu since we just did bn->relu->maxpool\n",
    "            conv_1_1 = Conv2D(filters=filters, kernel_size=(1, 1),\n",
    "                              strides=init_strides,\n",
    "                              padding=\"same\",\n",
    "                              kernel_initializer=\"he_normal\",\n",
    "                              kernel_regularizer=l2(1e-4))(input)\n",
    "        else:\n",
    "            conv_1_1 = _bn_relu_conv(filters=filters, kernel_size=(1, 1),\n",
    "                                     strides=init_strides)(input)\n",
    "\n",
    "        conv_3_3 = _bn_relu_conv(filters=filters, kernel_size=(3, 3))(conv_1_1)\n",
    "        residual = _bn_relu_conv(filters=filters * 4, kernel_size=(1, 1))(conv_3_3)\n",
    "        return _shortcut(input, residual)\n",
    "\n",
    "    return f\n",
    "\n",
    "\n",
    "def _handle_dim_ordering():\n",
    "    global ROW_AXIS\n",
    "    global COL_AXIS\n",
    "    global CHANNEL_AXIS\n",
    "    if K.image_dim_ordering() == 'tf':\n",
    "        ROW_AXIS = 1\n",
    "        COL_AXIS = 2\n",
    "        CHANNEL_AXIS = 3\n",
    "    else:\n",
    "        CHANNEL_AXIS = 1\n",
    "        ROW_AXIS = 2\n",
    "        COL_AXIS = 3\n",
    "\n",
    "\n",
    "def _get_block(identifier):\n",
    "    if isinstance(identifier, six.string_types):\n",
    "        res = globals().get(identifier)\n",
    "        if not res:\n",
    "            raise ValueError('Invalid {}'.format(identifier))\n",
    "        return res\n",
    "    return identifier\n",
    "\n",
    "\n",
    "class ResnetBuilder(object):\n",
    "    @staticmethod\n",
    "    def build(input_shape, num_outputs, block_fn, repetitions):\n",
    "        \"\"\"Builds a custom ResNet like architecture.\n",
    "        Args:\n",
    "            input_shape: The input shape in the form (nb_channels, nb_rows, nb_cols)\n",
    "            num_outputs: The number of outputs at final softmax layer\n",
    "            block_fn: The block function to use. This is either `basic_block` or `bottleneck`.\n",
    "                The original paper used basic_block for layers < 50\n",
    "            repetitions: Number of repetitions of various block units.\n",
    "                At each block unit, the number of filters are doubled and the input size is halved\n",
    "        Returns:\n",
    "            The keras `Model`.\n",
    "        \"\"\"\n",
    "        _handle_dim_ordering()\n",
    "        if len(input_shape) != 3:\n",
    "            raise Exception(\"Input shape should be a tuple (nb_channels, nb_rows, nb_cols)\")\n",
    "\n",
    "        # Permute dimension order if necessary\n",
    "        if K.image_dim_ordering() == 'tf':\n",
    "            input_shape = (input_shape[1], input_shape[2], input_shape[0])\n",
    "\n",
    "        # Load function from str if needed.\n",
    "        block_fn = _get_block(block_fn)\n",
    "\n",
    "        input = Input(shape=input_shape)\n",
    "        conv1 = _conv_bn_relu(filters=64, kernel_size=(7, 7), strides=(2, 2))(input)\n",
    "        pool1 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding=\"same\")(conv1)\n",
    "\n",
    "        block = pool1\n",
    "        filters = 64\n",
    "        for i, r in enumerate(repetitions):\n",
    "            block = _residual_block(block_fn, filters=filters, repetitions=r, is_first_layer=(i == 0))(block)\n",
    "            filters *= 2\n",
    "\n",
    "        # Last activation\n",
    "        block = _bn_relu(block)\n",
    "\n",
    "        # Classifier block\n",
    "        block_shape = K.int_shape(block)\n",
    "        pool2 = AveragePooling2D(pool_size=(block_shape[ROW_AXIS], block_shape[COL_AXIS]),\n",
    "                                 strides=(1, 1))(block)\n",
    "        flatten1 = Flatten()(pool2)\n",
    "        dense = Dense(units=num_outputs, kernel_initializer=\"he_normal\",\n",
    "                      activation=\"softmax\")(flatten1)\n",
    "\n",
    "        model = Model(inputs=input, outputs=dense)\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def build_resnet_18(input_shape, num_outputs):\n",
    "        return ResnetBuilder.build(input_shape, num_outputs, basic_block, [2, 2, 2, 2])\n",
    "\n",
    "    @staticmethod\n",
    "    def build_resnet_34(input_shape, num_outputs):\n",
    "        return ResnetBuilder.build(input_shape, num_outputs, basic_block, [3, 4, 6, 3])\n",
    "\n",
    "    @staticmethod\n",
    "    def build_resnet_50(input_shape, num_outputs):\n",
    "        return ResnetBuilder.build(input_shape, num_outputs, bottleneck, [3, 4, 6, 3])\n",
    "\n",
    "    @staticmethod\n",
    "    def build_resnet_101(input_shape, num_outputs):\n",
    "        return ResnetBuilder.build(input_shape, num_outputs, bottleneck, [3, 4, 23, 3])\n",
    "\n",
    "    @staticmethod\n",
    "    def build_resnet_152(input_shape, num_outputs):\n",
    "        return ResnetBuilder.build(input_shape, num_outputs, bottleneck, [3, 8, 36, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import *\n",
    "\n",
    "class CyclicLR(Callback):\n",
    "    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n",
    "    The method cycles the learning rate between two boundaries with\n",
    "    some constant frequency, as detailed in this paper (https://arxiv.org/abs/1506.01186).\n",
    "    The amplitude of the cycle can be scaled on a per-iteration or \n",
    "    per-cycle basis.\n",
    "    This class has three built-in policies, as put forth in the paper.\n",
    "    \"triangular\":\n",
    "        A basic triangular cycle w/ no amplitude scaling.\n",
    "    \"triangular2\":\n",
    "        A basic triangular cycle that scales initial amplitude by half each cycle.\n",
    "    \"exp_range\":\n",
    "        A cycle that scales initial amplitude by gamma**(cycle iterations) at each \n",
    "        cycle iteration.\n",
    "    For more detail, please see paper.\n",
    "    \n",
    "    # Example\n",
    "        ```python\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., mode='triangular')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```\n",
    "    \n",
    "    Class also supports custom scaling functions:\n",
    "        ```python\n",
    "            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., scale_fn=clr_fn,\n",
    "                                scale_mode='cycle')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```    \n",
    "    # Arguments\n",
    "        base_lr: initial learning rate which is the\n",
    "            lower boundary in the cycle.\n",
    "        max_lr: upper boundary in the cycle. Functionally,\n",
    "            it defines the cycle amplitude (max_lr - base_lr).\n",
    "            The lr at any cycle is the sum of base_lr\n",
    "            and some scaling of the amplitude; therefore \n",
    "            max_lr may not actually be reached depending on\n",
    "            scaling function.\n",
    "        step_size: number of training iterations per\n",
    "            half cycle. Authors suggest setting step_size\n",
    "            2-8 x training iterations in epoch.\n",
    "        mode: one of {triangular, triangular2, exp_range}.\n",
    "            Default 'triangular'.\n",
    "            Values correspond to policies detailed above.\n",
    "            If scale_fn is not None, this argument is ignored.\n",
    "        gamma: constant in 'exp_range' scaling function:\n",
    "            gamma**(cycle iterations)\n",
    "        scale_fn: Custom scaling policy defined by a single\n",
    "            argument lambda function, where \n",
    "            0 <= scale_fn(x) <= 1 for all x >= 0.\n",
    "            mode paramater is ignored \n",
    "        scale_mode: {'cycle', 'iterations'}.\n",
    "            Defines whether scale_fn is evaluated on \n",
    "            cycle number or cycle iterations (training\n",
    "            iterations since start of cycle). Default is 'cycle'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n",
    "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
    "        super(CyclicLR, self).__init__()\n",
    "\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "        if scale_fn == None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = lambda x: 1.\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = lambda x: 1/(2.**(x-1))\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = lambda x: gamma**(x)\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "        self.clr_iterations = 0.\n",
    "        self.trn_iterations = 0.\n",
    "        self.history = {}\n",
    "\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
    "               new_step_size=None):\n",
    "        \"\"\"Resets cycle iterations.\n",
    "        Optional boundary/step size adjustment.\n",
    "        \"\"\"\n",
    "        if new_base_lr != None:\n",
    "            self.base_lr = new_base_lr\n",
    "        if new_max_lr != None:\n",
    "            self.max_lr = new_max_lr\n",
    "        if new_step_size != None:\n",
    "            self.step_size = new_step_size\n",
    "        self.clr_iterations = 0.\n",
    "        \n",
    "    def clr(self):\n",
    "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
    "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
    "        if self.scale_mode == 'cycle':\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
    "        else:\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        logs = logs or {}\n",
    "\n",
    "        if self.clr_iterations == 0:\n",
    "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
    "        else:\n",
    "            K.set_value(self.model.optimizer.lr, self.clr())        \n",
    "            \n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        \n",
    "        logs = logs or {}\n",
    "        self.trn_iterations += 1\n",
    "        self.clr_iterations += 1\n",
    "\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "        \n",
    "        K.set_value(self.model.optimizer.lr, self.clr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=ResnetBuilder()\n",
    "model = x.build_resnet_18((3,64,64),200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n",
    "\n",
    "clr = CyclicLR(base_lr=1e-4, max_lr=1e-3,\n",
    "                    step_size=1562., scale_fn=clr_fn,\n",
    "                    scale_mode='cycle')\n",
    "\n",
    "# model.fit(X_train, Y_train, callbacks=[clr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 64, 64, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 64)   9472        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 64)   256         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 32, 64)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 16, 16, 64)   0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 16, 16, 64)   36928       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 16, 16, 64)   256         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 16, 16, 64)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 16, 16, 64)   36928       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 16, 16, 64)   0           max_pooling2d_1[0][0]            \n",
      "                                                                 conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 16, 16, 64)   256         add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 16, 16, 64)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 16, 16, 64)   36928       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 16, 16, 64)   256         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 16, 16, 64)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 16, 16, 64)   36928       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 16, 16, 64)   0           add_1[0][0]                      \n",
      "                                                                 conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 16, 16, 64)   256         add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 16, 16, 64)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 8, 8, 128)    73856       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 8, 8, 128)    512         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 8, 8, 128)    0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 8, 8, 128)    8320        add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 8, 8, 128)    147584      activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 8, 8, 128)    0           conv2d_8[0][0]                   \n",
      "                                                                 conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 8, 8, 128)    512         add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 8, 8, 128)    0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 8, 8, 128)    147584      activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 8, 8, 128)    512         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 8, 8, 128)    0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 8, 8, 128)    147584      activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 8, 8, 128)    0           add_3[0][0]                      \n",
      "                                                                 conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 8, 8, 128)    512         add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 8, 8, 128)    0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 4, 4, 256)    295168      activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 4, 4, 256)    1024        conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 4, 4, 256)    0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 4, 4, 256)    33024       add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 4, 4, 256)    590080      activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 4, 4, 256)    0           conv2d_13[0][0]                  \n",
      "                                                                 conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 4, 4, 256)    1024        add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 4, 4, 256)    0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 4, 4, 256)    590080      activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 4, 4, 256)    1024        conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 4, 4, 256)    0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 4, 4, 256)    590080      activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 4, 4, 256)    0           add_5[0][0]                      \n",
      "                                                                 conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 4, 4, 256)    1024        add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 4, 4, 256)    0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 2, 2, 512)    1180160     activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 2, 2, 512)    2048        conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 2, 2, 512)    0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 2, 2, 512)    131584      add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 2, 2, 512)    2359808     activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 2, 2, 512)    0           conv2d_18[0][0]                  \n",
      "                                                                 conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 2, 2, 512)    2048        add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 2, 2, 512)    0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 2, 2, 512)    2359808     activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 2, 2, 512)    2048        conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 2, 2, 512)    0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 2, 2, 512)    2359808     activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 2, 2, 512)    0           add_7[0][0]                      \n",
      "                                                                 conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 2, 2, 512)    2048        add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 2, 2, 512)    0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 1, 1, 512)    0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 512)          0           average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 200)          102600      flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 11,289,928\n",
      "Trainable params: 11,282,120\n",
      "Non-trainable params: 7,808\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100000 images belonging to 200 classes.\n",
      "Found 10000 images belonging to 200 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,horizontal_flip=0.5)\n",
    "\n",
    "test_datagen = ImageDataGenerator()\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        './train2',  # this is the target directory\n",
    "    target_size=(64,64),\n",
    "        batch_size=5,\n",
    "        class_mode='categorical')\n",
    "\n",
    "# this is a similar generator, for validation data\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        './val',target_size=(64,64),\n",
    "        batch_size=5,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/vgeta/Downloads/tiny-imagenet-200'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-e66b716cb24c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mSTEP_SIZE_TRAIN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mSTEP_SIZE_VALID\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m model.fit_generator(generator=train_generator,\n\u001b[1;32m      5\u001b[0m                     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSTEP_SIZE_TRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_generator' is not defined"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer = Adam(lr = 1e-4), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "STEP_SIZE_TRAIN=train_generator.n//train_generator.batch_size\n",
    "STEP_SIZE_VALID=validation_generator.n//validation_generator.batch_size\n",
    "model.fit_generator(generator=train_generator,\n",
    "                    steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps=STEP_SIZE_VALID,\n",
    "                    epochs=10,callbacks=[clr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (keras)",
   "language": "python",
   "name": "keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
