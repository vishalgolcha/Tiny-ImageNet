{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ResNet18 60 epochs",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "mC1yleuQTEx6",
        "colab_type": "code",
        "outputId": "9f3912d8-e2e7-4b08-eb51-2c117626c3a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "G_qA9yPcUtes",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cp \"/content/drive/My Drive/tiny_imgnet.zip\" \"/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z1nCP5mbU-ZT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!unzip -q '/tiny_imgnet.zip' "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1sJb1HNaWziK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os \n",
        "os.mkdir('/training2')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8DNFEYxoWhMH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf \n",
        "from __future__ import division\n",
        "from keras.optimizers import Adam\n",
        "from keras.optimizers import SGD\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras.layers.merge import concatenate\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import activations\n",
        "import numpy as np\n",
        "\n",
        "import six\n",
        "from keras.models import Model\n",
        "from keras.layers import (\n",
        "    Input,\n",
        "    Activation,\n",
        "    Dense,\n",
        "    Flatten,\n",
        "    Softmax\n",
        ")\n",
        "from keras.layers.convolutional import (\n",
        "    Conv2D,\n",
        "    MaxPooling2D,\n",
        "    AveragePooling2D\n",
        ")\n",
        "from keras.layers.merge import add\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K\n",
        "\n",
        "\n",
        "def _bn_relu(input):\n",
        "    \"\"\"Helper to build a BN -> relu block\n",
        "    \"\"\"\n",
        "    norm = BatchNormalization(axis=CHANNEL_AXIS)(input)\n",
        "    return Activation(\"relu\")(norm)\n",
        "\n",
        "\n",
        "def _conv_bn_relu(**conv_params):\n",
        "    \"\"\"Helper to build a conv -> BN -> relu block\n",
        "    \"\"\"\n",
        "    filters = conv_params[\"filters\"]\n",
        "    kernel_size = conv_params[\"kernel_size\"]\n",
        "    strides = conv_params.setdefault(\"strides\", (1, 1))\n",
        "    kernel_initializer = conv_params.setdefault(\"kernel_initializer\", \"he_normal\")\n",
        "    padding = conv_params.setdefault(\"padding\", \"same\")\n",
        "    kernel_regularizer = conv_params.setdefault(\"kernel_regularizer\", l2(1.e-4))\n",
        "\n",
        "    def f(input):\n",
        "        conv = Conv2D(filters=filters, kernel_size=kernel_size,\n",
        "                      strides=strides, padding=padding,\n",
        "                      kernel_initializer=kernel_initializer,\n",
        "                      kernel_regularizer=kernel_regularizer)(input)\n",
        "        return _bn_relu(conv)\n",
        "\n",
        "    return f\n",
        "\n",
        "\n",
        "def _bn_relu_conv(**conv_params):\n",
        "    \"\"\"Helper to build a BN -> relu -> conv block.\n",
        "    This is an improved scheme proposed in http://arxiv.org/pdf/1603.05027v2.pdf\n",
        "    \"\"\"\n",
        "    filters = conv_params[\"filters\"]\n",
        "    kernel_size = conv_params[\"kernel_size\"]\n",
        "    strides = conv_params.setdefault(\"strides\", (1, 1))\n",
        "    kernel_initializer = conv_params.setdefault(\"kernel_initializer\", \"he_normal\")\n",
        "    padding = conv_params.setdefault(\"padding\", \"same\")\n",
        "    kernel_regularizer = conv_params.setdefault(\"kernel_regularizer\", l2(1.e-4))\n",
        "\n",
        "    def f(input):\n",
        "        activation = _bn_relu(input)\n",
        "        return Conv2D(filters=filters, kernel_size=kernel_size,\n",
        "                      strides=strides, padding=padding,\n",
        "                      kernel_initializer=kernel_initializer,\n",
        "                      kernel_regularizer=kernel_regularizer)(activation)\n",
        "\n",
        "    return f\n",
        "\n",
        "\n",
        "def _shortcut(input, residual):\n",
        "    \"\"\"Adds a shortcut between input and residual block and merges them with \"sum\"\n",
        "    \"\"\"\n",
        "    # Expand channels of shortcut to match residual.\n",
        "    # Stride appropriately to match residual (width, height)\n",
        "    # Should be int if network architecture is correctly configured.\n",
        "    input_shape = K.int_shape(input)\n",
        "    residual_shape = K.int_shape(residual)\n",
        "    stride_width = int(round(input_shape[ROW_AXIS] / residual_shape[ROW_AXIS]))\n",
        "    stride_height = int(round(input_shape[COL_AXIS] / residual_shape[COL_AXIS]))\n",
        "    equal_channels = input_shape[CHANNEL_AXIS] == residual_shape[CHANNEL_AXIS]\n",
        "\n",
        "    shortcut = input\n",
        "    # 1 X 1 conv if shape is different. Else identity.\n",
        "    if stride_width > 1 or stride_height > 1 or not equal_channels:\n",
        "        shortcut = Conv2D(filters=residual_shape[CHANNEL_AXIS],\n",
        "                          kernel_size=(1, 1),\n",
        "                          strides=(stride_width, stride_height),\n",
        "                          padding=\"valid\",\n",
        "                          kernel_initializer=\"he_normal\",\n",
        "                          kernel_regularizer=l2(0.0001))(input)\n",
        "\n",
        "    return add([shortcut, residual])\n",
        "\n",
        "\n",
        "def _residual_block(block_function, filters, repetitions, is_first_layer=False):\n",
        "    \"\"\"Builds a residual block with repeating bottleneck blocks.\n",
        "    \"\"\"\n",
        "    def f(input):\n",
        "        for i in range(repetitions):\n",
        "            init_strides = (1, 1)\n",
        "            if i == 0 and not is_first_layer:\n",
        "                init_strides = (2, 2)\n",
        "            input = block_function(filters=filters, init_strides=init_strides,\n",
        "                                   is_first_block_of_first_layer=(is_first_layer and i == 0))(input)\n",
        "        return input\n",
        "\n",
        "    return f\n",
        "\n",
        "\n",
        "def basic_block(filters, init_strides=(1, 1), is_first_block_of_first_layer=False):\n",
        "    \"\"\"Basic 3 X 3 convolution blocks for use on resnets with layers <= 34.\n",
        "    Follows improved proposed scheme in http://arxiv.org/pdf/1603.05027v2.pdf\n",
        "    \"\"\"\n",
        "    def f(input):\n",
        "\n",
        "        if is_first_block_of_first_layer:\n",
        "            # don't repeat bn->relu since we just did bn->relu->maxpool\n",
        "            conv1 = Conv2D(filters=filters, kernel_size=(3, 3),\n",
        "                           strides=init_strides,\n",
        "                           padding=\"same\",\n",
        "                           kernel_initializer=\"he_normal\",\n",
        "                           kernel_regularizer=l2(1e-4))(input)\n",
        "        else:\n",
        "            conv1 = _bn_relu_conv(filters=filters, kernel_size=(3, 3),\n",
        "                                  strides=init_strides)(input)\n",
        "\n",
        "        residual = _bn_relu_conv(filters=filters, kernel_size=(3, 3))(conv1)\n",
        "        return _shortcut(input, residual)\n",
        "\n",
        "    return f\n",
        "\n",
        "\n",
        "def bottleneck(filters, init_strides=(1, 1), is_first_block_of_first_layer=False):\n",
        "    \"\"\"Bottleneck architecture for > 34 layer resnet.\n",
        "    Follows improved proposed scheme in http://arxiv.org/pdf/1603.05027v2.pdf\n",
        "    Returns:\n",
        "        A final conv layer of filters * 4\n",
        "    \"\"\"\n",
        "    def f(input):\n",
        "\n",
        "        if is_first_block_of_first_layer:\n",
        "            # don't repeat bn->relu since we just did bn->relu->maxpool\n",
        "            conv_1_1 = Conv2D(filters=filters, kernel_size=(1, 1),\n",
        "                              strides=init_strides,\n",
        "                              padding=\"same\",\n",
        "                              kernel_initializer=\"he_normal\",\n",
        "                              kernel_regularizer=l2(1e-4))(input)\n",
        "        else:\n",
        "            conv_1_1 = _bn_relu_conv(filters=filters, kernel_size=(1, 1),\n",
        "                                     strides=init_strides)(input)\n",
        "\n",
        "        conv_3_3 = _bn_relu_conv(filters=filters, kernel_size=(3, 3))(conv_1_1)\n",
        "        residual = _bn_relu_conv(filters=filters * 4, kernel_size=(1, 1))(conv_3_3)\n",
        "        return _shortcut(input, residual)\n",
        "\n",
        "    return f\n",
        "\n",
        "\n",
        "def _handle_dim_ordering():\n",
        "    global ROW_AXIS\n",
        "    global COL_AXIS\n",
        "    global CHANNEL_AXIS\n",
        "    if K.image_dim_ordering() == 'tf':\n",
        "        ROW_AXIS = 1\n",
        "        COL_AXIS = 2\n",
        "        CHANNEL_AXIS = 3\n",
        "    else:\n",
        "        CHANNEL_AXIS = 1\n",
        "        ROW_AXIS = 2\n",
        "        COL_AXIS = 3\n",
        "\n",
        "\n",
        "def _get_block(identifier):\n",
        "    if isinstance(identifier, six.string_types):\n",
        "        res = globals().get(identifier)\n",
        "        if not res:\n",
        "            raise ValueError('Invalid {}'.format(identifier))\n",
        "        return res\n",
        "    return identifier\n",
        "\n",
        "\n",
        "class ResnetBuilder(object):\n",
        "    @staticmethod\n",
        "    def build(input_shape, num_outputs, block_fn, repetitions):\n",
        "        \"\"\"Builds a custom ResNet like architecture.\n",
        "        Args:\n",
        "            input_shape: The input shape in the form (nb_channels, nb_rows, nb_cols)\n",
        "            num_outputs: The number of outputs at final softmax layer\n",
        "            block_fn: The block function to use. This is either `basic_block` or `bottleneck`.\n",
        "                The original paper used basic_block for layers < 50\n",
        "            repetitions: Number of repetitions of various block units.\n",
        "                At each block unit, the number of filters are doubled and the input size is halved\n",
        "        Returns:\n",
        "            The keras `Model`.\n",
        "        \"\"\"\n",
        "        _handle_dim_ordering()\n",
        "        if len(input_shape) != 3:\n",
        "            raise Exception(\"Input shape should be a tuple (nb_channels, nb_rows, nb_cols)\")\n",
        "\n",
        "        # Permute dimension order if necessary\n",
        "        if K.image_dim_ordering() == 'tf':\n",
        "            input_shape = (input_shape[1], input_shape[2], input_shape[0])\n",
        "\n",
        "        # Load function from str if needed.\n",
        "        block_fn = _get_block(block_fn)\n",
        "\n",
        "        input = Input(shape=input_shape)\n",
        "        conv1 = _conv_bn_relu(filters=64, kernel_size=(7, 7), strides=(2, 2))(input)\n",
        "        pool1 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding=\"same\")(conv1)\n",
        "\n",
        "        block = pool1\n",
        "        filters = 64\n",
        "        for i, r in enumerate(repetitions):\n",
        "            block = _residual_block(block_fn, filters=filters, repetitions=r, is_first_layer=(i == 0))(block)\n",
        "            filters *= 2\n",
        "\n",
        "        # Last activation\n",
        "        block = _bn_relu(block)\n",
        "\n",
        "        # Classifier block\n",
        "        block_shape = K.int_shape(block)\n",
        "        pool2 = AveragePooling2D(pool_size=(block_shape[ROW_AXIS], block_shape[COL_AXIS]),\n",
        "                                 strides=(1, 1))(block)\n",
        "        fini  = Conv2D(filters=200, kernel_size=(1, 1),\n",
        "                              strides=(1,1),\n",
        "                              kernel_initializer=\"he_normal\",\n",
        "                              kernel_regularizer=l2(1e-4))(pool2)\n",
        "        flatten1 = Flatten()(fini)\n",
        "        fini2 = Softmax(axis=-1)(flatten1)\n",
        "        \n",
        "#         dense = Dense(units=num_outputs, kernel_initializer=\"he_normal\",\n",
        "#                       activation=\"softmax\")(flatten1)\n",
        "\n",
        "        model = Model(inputs=input, outputs=fini2)\n",
        "        return model\n",
        "\n",
        "    @staticmethod\n",
        "    def build_resnet_18(input_shape, num_outputs):\n",
        "        return ResnetBuilder.build(input_shape, num_outputs, basic_block, [2, 2, 2, 2])\n",
        "\n",
        "    @staticmethod\n",
        "    def build_resnet_34(input_shape, num_outputs):\n",
        "        return ResnetBuilder.build(input_shape, num_outputs, basic_block, [3, 4, 6, 3])\n",
        "\n",
        "    @staticmethod\n",
        "    def build_resnet_50(input_shape, num_outputs):\n",
        "        return ResnetBuilder.build(input_shape, num_outputs, bottleneck, [3, 4, 6, 3])\n",
        "\n",
        "    @staticmethod\n",
        "    def build_resnet_101(input_shape, num_outputs):\n",
        "        return ResnetBuilder.build(input_shape, num_outputs, bottleneck, [3, 4, 23, 3])\n",
        "\n",
        "    @staticmethod\n",
        "    def build_resnet_152(input_shape, num_outputs):\n",
        "        return ResnetBuilder.build(input_shape, num_outputs, bottleneck, [3, 8, 36, 3])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9IpZlS4bW3F_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.callbacks import *\n",
        "\n",
        "class CyclicLR(Callback):\n",
        "    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n",
        "    The method cycles the learning rate between two boundaries with\n",
        "    some constant frequency, as detailed in this paper (https://arxiv.org/abs/1506.01186).\n",
        "    The amplitude of the cycle can be scaled on a per-iteration or \n",
        "    per-cycle basis.\n",
        "    This class has three built-in policies, as put forth in the paper.\n",
        "    \"triangular\":\n",
        "        A basic triangular cycle w/ no amplitude scaling.\n",
        "    \"triangular2\":\n",
        "        A basic triangular cycle that scales initial amplitude by half each cycle.\n",
        "    \"exp_range\":\n",
        "        A cycle that scales initial amplitude by gamma**(cycle iterations) at each \n",
        "        cycle iteration.\n",
        "    For more detail, please see paper.\n",
        "    \n",
        "    # Example\n",
        "        ```python\n",
        "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
        "                                step_size=2000., mode='triangular')\n",
        "            model.fit(X_train, Y_train, callbacks=[clr])\n",
        "        ```\n",
        "    \n",
        "    Class also supports custom scaling functions:\n",
        "        ```python\n",
        "            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n",
        "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
        "                                step_size=2000., scale_fn=clr_fn,\n",
        "                                scale_mode='cycle')\n",
        "            model.fit(X_train, Y_train, callbacks=[clr])\n",
        "        ```    \n",
        "    # Arguments\n",
        "        base_lr: initial learning rate which is the\n",
        "            lower boundary in the cycle.\n",
        "        max_lr: upper boundary in the cycle. Functionally,\n",
        "            it defines the cycle amplitude (max_lr - base_lr).\n",
        "            The lr at any cycle is the sum of base_lr\n",
        "            and some scaling of the amplitude; therefore \n",
        "            max_lr may not actually be reached depending on\n",
        "            scaling function.\n",
        "        step_size: number of training iterations per\n",
        "            half cycle. Authors suggest setting step_size\n",
        "            2-8 x training iterations in epoch.\n",
        "        mode: one of {triangular, triangular2, exp_range}.\n",
        "            Default 'triangular'.\n",
        "            Values correspond to policies detailed above.\n",
        "            If scale_fn is not None, this argument is ignored.\n",
        "        gamma: constant in 'exp_range' scaling function:\n",
        "            gamma**(cycle iterations)\n",
        "        scale_fn: Custom scaling policy defined by a single\n",
        "            argument lambda function, where \n",
        "            0 <= scale_fn(x) <= 1 for all x >= 0.\n",
        "            mode paramater is ignored \n",
        "        scale_mode: {'cycle', 'iterations'}.\n",
        "            Defines whether scale_fn is evaluated on \n",
        "            cycle number or cycle iterations (training\n",
        "            iterations since start of cycle). Default is 'cycle'.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n",
        "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
        "        super(CyclicLR, self).__init__()\n",
        "\n",
        "        self.base_lr = base_lr\n",
        "        self.max_lr = max_lr\n",
        "        self.step_size = step_size\n",
        "        self.mode = mode\n",
        "        self.gamma = gamma\n",
        "        if scale_fn == None:\n",
        "            if self.mode == 'triangular':\n",
        "                self.scale_fn = lambda x: 1.\n",
        "                self.scale_mode = 'cycle'\n",
        "            elif self.mode == 'triangular2':\n",
        "                self.scale_fn = lambda x: 1/(2.**(x-1))\n",
        "                self.scale_mode = 'cycle'\n",
        "            elif self.mode == 'exp_range':\n",
        "                self.scale_fn = lambda x: gamma**(x)\n",
        "                self.scale_mode = 'iterations'\n",
        "        else:\n",
        "            self.scale_fn = scale_fn\n",
        "            self.scale_mode = scale_mode\n",
        "        self.clr_iterations = 0.\n",
        "        self.trn_iterations = 0.\n",
        "        self.history = {}\n",
        "\n",
        "        self._reset()\n",
        "\n",
        "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
        "               new_step_size=None):\n",
        "        \"\"\"Resets cycle iterations.\n",
        "        Optional boundary/step size adjustment.\n",
        "        \"\"\"\n",
        "        if new_base_lr != None:\n",
        "            self.base_lr = new_base_lr\n",
        "        if new_max_lr != None:\n",
        "            self.max_lr = new_max_lr\n",
        "        if new_step_size != None:\n",
        "            self.step_size = new_step_size\n",
        "        self.clr_iterations = 0.\n",
        "        \n",
        "    def clr(self):\n",
        "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
        "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
        "        if self.scale_mode == 'cycle':\n",
        "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
        "        else:\n",
        "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n",
        "        \n",
        "    def on_train_begin(self, logs={}):\n",
        "        logs = logs or {}\n",
        "\n",
        "        if self.clr_iterations == 0:\n",
        "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
        "        else:\n",
        "            K.set_value(self.model.optimizer.lr, self.clr())        \n",
        "            \n",
        "    def on_batch_end(self, epoch, logs=None):\n",
        "        \n",
        "        logs = logs or {}\n",
        "        self.trn_iterations += 1\n",
        "        self.clr_iterations += 1\n",
        "\n",
        "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
        "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
        "\n",
        "        for k, v in logs.items():\n",
        "            self.history.setdefault(k, []).append(v)\n",
        "        \n",
        "        K.set_value(self.model.optimizer.lr, self.clr())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5PhTAvNdW4-O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n",
        "\n",
        "clr = CyclicLR(base_lr=1e-4, max_lr=1e-3,\n",
        "                    step_size=1562., scale_fn=clr_fn,\n",
        "                    scale_mode='cycle')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9uInopfwZAN0",
        "colab_type": "code",
        "outputId": "560c2158-92bd-4a1e-bf95-51d7c0f87886",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2711
        }
      },
      "cell_type": "code",
      "source": [
        "x=ResnetBuilder()\n",
        "model = x.build_resnet_18((3,64,64),200)\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 64, 64, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 32, 32, 64)   9472        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 32, 32, 64)   256         conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 32, 32, 64)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 16, 16, 64)   0           activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 16, 16, 64)   36928       max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 16, 16, 64)   256         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 16, 16, 64)   0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 16, 16, 64)   36928       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 16, 16, 64)   0           max_pooling2d_1[0][0]            \n",
            "                                                                 conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 16, 16, 64)   256         add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 16, 16, 64)   0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 16, 16, 64)   36928       activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 16, 16, 64)   256         conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 16, 16, 64)   0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 16, 16, 64)   36928       activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 16, 16, 64)   0           add_1[0][0]                      \n",
            "                                                                 conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 16, 16, 64)   256         add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 16, 16, 64)   0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 8, 8, 128)    73856       activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 8, 8, 128)    512         conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 8, 8, 128)    0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 8, 8, 128)    8320        add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 8, 8, 128)    147584      activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 8, 8, 128)    0           conv2d_8[0][0]                   \n",
            "                                                                 conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 8, 8, 128)    512         add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 8, 8, 128)    0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 8, 8, 128)    147584      activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 8, 8, 128)    512         conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 8, 8, 128)    0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 8, 8, 128)    147584      activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 8, 8, 128)    0           add_3[0][0]                      \n",
            "                                                                 conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 8, 8, 128)    512         add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 8, 8, 128)    0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 4, 4, 256)    295168      activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 4, 4, 256)    1024        conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 4, 4, 256)    0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 4, 4, 256)    33024       add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 4, 4, 256)    590080      activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 4, 4, 256)    0           conv2d_13[0][0]                  \n",
            "                                                                 conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 4, 4, 256)    1024        add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 4, 4, 256)    0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 4, 4, 256)    590080      activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 4, 4, 256)    1024        conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 4, 4, 256)    0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 4, 4, 256)    590080      activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 4, 4, 256)    0           add_5[0][0]                      \n",
            "                                                                 conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 4, 4, 256)    1024        add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 4, 4, 256)    0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 2, 2, 512)    1180160     activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 2, 2, 512)    2048        conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 2, 2, 512)    0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 2, 2, 512)    131584      add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 2, 2, 512)    2359808     activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_7 (Add)                     (None, 2, 2, 512)    0           conv2d_18[0][0]                  \n",
            "                                                                 conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 2, 2, 512)    2048        add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 2, 2, 512)    0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 2, 2, 512)    2359808     activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 2, 2, 512)    2048        conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 2, 2, 512)    0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 2, 2, 512)    2359808     activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_8 (Add)                     (None, 2, 2, 512)    0           add_7[0][0]                      \n",
            "                                                                 conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 2, 2, 512)    2048        add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 2, 2, 512)    0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (None, 1, 1, 512)    0           activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 1, 1, 200)    102600      average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 200)          0           conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "softmax_1 (Softmax)             (None, 200)          0           flatten_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 11,289,928\n",
            "Trainable params: 11,282,120\n",
            "Non-trainable params: 7,808\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GOhpJCD8nc5g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#random cropping and blackouts \n",
        "\n",
        "def random_blackout(img,bout):\n",
        "    # bout is a tuple for the size of the blackout you want \n",
        "    # Note: image_data_format is 'channel_last'\n",
        "    assert img.shape[2] == 3\n",
        "    height, width = img.shape[0],img.shape[1]\n",
        "    dy, dx = bout\n",
        "    x = np.random.randint(0, width-1 )\n",
        "    y = np.random.randint(0, height-1)\n",
        "    \n",
        "    demon = np.random.rand(0,1)\n",
        "    \n",
        "    if demon >0.6:    \n",
        "      img[y:min((y+dy),img.shape[1]),x:min((x+dx),img.shape[0]),:]=0  \n",
        "    \n",
        "    return img\n",
        "\n",
        "\n",
        "def bout_generator(batches, bout_length):\n",
        "    while True:\n",
        "        batch_x, batch_y = next(batches)\n",
        "        batch_crops = np.zeros((batch_x.shape[0],64,64,3))\n",
        "        for i in range(batch_x.shape[0]):\n",
        "            batch_crops[i] = random_blackout(batch_x[i], (bout_length,bout_length))\n",
        "        yield (batch_crops, batch_y)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "goGVhgPIY99N",
        "colab_type": "code",
        "outputId": "7dc56064-5b26-4dcc-a159-2b28c337ea58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,horizontal_flip=0.5,zoom_range=[0.9,1.1])\n",
        "\n",
        "\n",
        "test_datagen = ImageDataGenerator()\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        './tiny_imgnet/train2',  # this is the target directory\n",
        "    target_size=(64,64),\n",
        "        batch_size=128,\n",
        "        class_mode='categorical')\n",
        "\n",
        "# this is a similar generator, for validation data\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "        './tiny_imgnet/val',target_size=(64,64),\n",
        "        batch_size=128,\n",
        "        class_mode='categorical')\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 100000 images belonging to 200 classes.\n",
            "Found 10000 images belonging to 200 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4FtxRVXqkx2W",
        "colab_type": "code",
        "outputId": "0b6dd506-89fa-4ff9-8829-83b68c54a6b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "yZ0ezq3HY1hq",
        "colab_type": "code",
        "outputId": "fb2a6590-646f-4329-bc51-c69be95b52fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2607
        }
      },
      "cell_type": "code",
      "source": [
        "checkpoint_path = \"/training2/cp-{epoch:04d}.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    checkpoint_path, verbose=1, save_weights_only=False,\n",
        "    period=5)\n",
        "\n",
        "model.save(checkpoint_path.format(epoch=0))\n",
        "\n",
        "model.compile(optimizer = Adam(), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "STEP_SIZE_TRAIN=train_generator.n//train_generator.batch_size\n",
        "STEP_SIZE_VALID=validation_generator.n//validation_generator.batch_size\n",
        "model.fit_generator(generator=train_generator,\n",
        "                    steps_per_epoch=STEP_SIZE_TRAIN,\n",
        "                    validation_data=validation_generator,\n",
        "                    validation_steps=STEP_SIZE_VALID,\n",
        "                    epochs=60,callbacks=[clr,cp_callback],shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/60\n",
            "781/781 [==============================] - 327s 418ms/step - loss: 5.4552 - acc: 0.0778 - val_loss: 5.0742 - val_acc: 0.1039\n",
            "Epoch 2/60\n",
            "781/781 [==============================] - 320s 410ms/step - loss: 4.5436 - acc: 0.1551 - val_loss: 4.7390 - val_acc: 0.1250\n",
            "Epoch 3/60\n",
            "781/781 [==============================] - 323s 414ms/step - loss: 4.0488 - acc: 0.2123 - val_loss: 4.1510 - val_acc: 0.2017\n",
            "Epoch 4/60\n",
            "781/781 [==============================] - 324s 414ms/step - loss: 3.6034 - acc: 0.2831 - val_loss: 3.4954 - val_acc: 0.3015\n",
            "Epoch 5/60\n",
            "781/781 [==============================] - 327s 418ms/step - loss: 3.3717 - acc: 0.3227 - val_loss: 3.6201 - val_acc: 0.2789\n",
            "\n",
            "Epoch 00005: saving model to /training2/cp-0005.ckpt\n",
            "Epoch 6/60\n",
            "781/781 [==============================] - 327s 418ms/step - loss: 3.4233 - acc: 0.3109 - val_loss: 3.9518 - val_acc: 0.2332\n",
            "Epoch 7/60\n",
            "781/781 [==============================] - 327s 418ms/step - loss: 3.3335 - acc: 0.3294 - val_loss: 3.5222 - val_acc: 0.2938\n",
            "Epoch 8/60\n",
            "781/781 [==============================] - 329s 421ms/step - loss: 3.0617 - acc: 0.3812 - val_loss: 3.0472 - val_acc: 0.3835\n",
            "Epoch 9/60\n",
            "781/781 [==============================] - 328s 420ms/step - loss: 2.8565 - acc: 0.4191 - val_loss: 3.0374 - val_acc: 0.3855\n",
            "Epoch 10/60\n",
            "781/781 [==============================] - 326s 417ms/step - loss: 2.7844 - acc: 0.4307 - val_loss: 3.0811 - val_acc: 0.3744\n",
            "\n",
            "Epoch 00010: saving model to /training2/cp-0010.ckpt\n",
            "Epoch 11/60\n",
            "781/781 [==============================] - 326s 417ms/step - loss: 2.7253 - acc: 0.4433 - val_loss: 3.0267 - val_acc: 0.3852\n",
            "Epoch 12/60\n",
            "781/781 [==============================] - 327s 419ms/step - loss: 2.6681 - acc: 0.4521 - val_loss: 3.0802 - val_acc: 0.3758\n",
            "Epoch 13/60\n",
            "781/781 [==============================] - 318s 407ms/step - loss: 2.7361 - acc: 0.4381 - val_loss: 3.6488 - val_acc: 0.2943\n",
            "Epoch 14/60\n",
            "781/781 [==============================] - 326s 418ms/step - loss: 2.9529 - acc: 0.3962 - val_loss: 3.3697 - val_acc: 0.3368\n",
            "Epoch 15/60\n",
            "781/781 [==============================] - 325s 416ms/step - loss: 2.9419 - acc: 0.4053 - val_loss: 3.3003 - val_acc: 0.3550\n",
            "\n",
            "Epoch 00015: saving model to /training2/cp-0015.ckpt\n",
            "Epoch 16/60\n",
            "781/781 [==============================] - 322s 412ms/step - loss: 2.6853 - acc: 0.4595 - val_loss: 2.8612 - val_acc: 0.4354\n",
            "Epoch 17/60\n",
            "781/781 [==============================] - 323s 414ms/step - loss: 2.7014 - acc: 0.4544 - val_loss: 3.8396 - val_acc: 0.2828\n",
            "Epoch 18/60\n",
            "781/781 [==============================] - 324s 415ms/step - loss: 3.0868 - acc: 0.3839 - val_loss: 4.1410 - val_acc: 0.2268\n",
            "Epoch 19/60\n",
            "781/781 [==============================] - 316s 405ms/step - loss: 3.0874 - acc: 0.3933 - val_loss: 3.3996 - val_acc: 0.3480\n",
            "Epoch 20/60\n",
            "781/781 [==============================] - 322s 412ms/step - loss: 2.7264 - acc: 0.4664 - val_loss: 2.9167 - val_acc: 0.4292\n",
            "\n",
            "Epoch 00020: saving model to /training2/cp-0020.ckpt\n",
            "Epoch 21/60\n",
            "781/781 [==============================] - 327s 418ms/step - loss: 2.5127 - acc: 0.5075 - val_loss: 3.2465 - val_acc: 0.3724\n",
            "Epoch 22/60\n",
            "781/781 [==============================] - 326s 418ms/step - loss: 2.6783 - acc: 0.4696 - val_loss: 4.3302 - val_acc: 0.2527\n",
            "Epoch 23/60\n",
            "781/781 [==============================] - 326s 418ms/step - loss: 2.6839 - acc: 0.4716 - val_loss: 3.0652 - val_acc: 0.4155\n",
            "Epoch 24/60\n",
            "781/781 [==============================] - 327s 418ms/step - loss: 2.4145 - acc: 0.5303 - val_loss: 2.7314 - val_acc: 0.4712\n",
            "Epoch 25/60\n",
            "781/781 [==============================] - 326s 417ms/step - loss: 2.2009 - acc: 0.5761 - val_loss: 2.7781 - val_acc: 0.4635\n",
            "\n",
            "Epoch 00025: saving model to /training2/cp-0025.ckpt\n",
            "Epoch 26/60\n",
            "781/781 [==============================] - 326s 417ms/step - loss: 2.1424 - acc: 0.5875 - val_loss: 2.7551 - val_acc: 0.4754\n",
            "Epoch 27/60\n",
            "781/781 [==============================] - 326s 417ms/step - loss: 2.0759 - acc: 0.5990 - val_loss: 2.7589 - val_acc: 0.4678\n",
            "Epoch 28/60\n",
            "781/781 [==============================] - 328s 420ms/step - loss: 2.0304 - acc: 0.6092 - val_loss: 2.7456 - val_acc: 0.4697\n",
            "Epoch 29/60\n",
            "781/781 [==============================] - 330s 423ms/step - loss: 2.1181 - acc: 0.5838 - val_loss: 3.1654 - val_acc: 0.4032\n",
            "Epoch 30/60\n",
            "781/781 [==============================] - 325s 416ms/step - loss: 2.4148 - acc: 0.5213 - val_loss: 3.6345 - val_acc: 0.3377\n",
            "\n",
            "Epoch 00030: saving model to /training2/cp-0030.ckpt\n",
            "Epoch 31/60\n",
            "781/781 [==============================] - 326s 418ms/step - loss: 2.4631 - acc: 0.5171 - val_loss: 3.1500 - val_acc: 0.4022\n",
            "Epoch 32/60\n",
            "781/781 [==============================] - 328s 419ms/step - loss: 2.1712 - acc: 0.5859 - val_loss: 2.7359 - val_acc: 0.4839\n",
            "Epoch 33/60\n",
            "781/781 [==============================] - 329s 421ms/step - loss: 2.2117 - acc: 0.5743 - val_loss: 3.4862 - val_acc: 0.3761\n",
            "Epoch 34/60\n",
            "781/781 [==============================] - 336s 430ms/step - loss: 2.7327 - acc: 0.4707 - val_loss: 3.6482 - val_acc: 0.3417\n",
            "Epoch 35/60\n",
            "781/781 [==============================] - 328s 419ms/step - loss: 2.7960 - acc: 0.4719 - val_loss: 3.1879 - val_acc: 0.4207\n",
            "\n",
            "Epoch 00035: saving model to /training2/cp-0035.ckpt\n",
            "Epoch 36/60\n",
            "781/781 [==============================] - 327s 419ms/step - loss: 2.4011 - acc: 0.5551 - val_loss: 2.7936 - val_acc: 0.4889\n",
            "Epoch 37/60\n",
            "781/781 [==============================] - 327s 419ms/step - loss: 2.1582 - acc: 0.6074 - val_loss: 3.1276 - val_acc: 0.4434\n",
            "Epoch 38/60\n",
            "781/781 [==============================] - 328s 420ms/step - loss: 2.3551 - acc: 0.5587 - val_loss: 3.4085 - val_acc: 0.3860\n",
            "Epoch 39/60\n",
            "781/781 [==============================] - 326s 418ms/step - loss: 2.3960 - acc: 0.5545 - val_loss: 3.0850 - val_acc: 0.4443\n",
            "Epoch 40/60\n",
            "781/781 [==============================] - 335s 428ms/step - loss: 2.0982 - acc: 0.6219 - val_loss: 2.7803 - val_acc: 0.4996\n",
            "\n",
            "Epoch 00040: saving model to /training2/cp-0040.ckpt\n",
            "Epoch 41/60\n",
            "781/781 [==============================] - 325s 416ms/step - loss: 1.8653 - acc: 0.6774 - val_loss: 2.8667 - val_acc: 0.4787\n",
            "Epoch 42/60\n",
            "781/781 [==============================] - 327s 418ms/step - loss: 1.8025 - acc: 0.6905 - val_loss: 2.8387 - val_acc: 0.4882\n",
            "Epoch 43/60\n",
            "781/781 [==============================] - 326s 417ms/step - loss: 1.7353 - acc: 0.7037 - val_loss: 2.8712 - val_acc: 0.4809\n",
            "Epoch 44/60\n",
            "781/781 [==============================] - 326s 418ms/step - loss: 1.6856 - acc: 0.7120 - val_loss: 2.9012 - val_acc: 0.4865\n",
            "Epoch 45/60\n",
            "781/781 [==============================] - 326s 418ms/step - loss: 1.7897 - acc: 0.6797 - val_loss: 3.2750 - val_acc: 0.4412\n",
            "\n",
            "Epoch 00045: saving model to /training2/cp-0045.ckpt\n",
            "Epoch 46/60\n",
            "781/781 [==============================] - 324s 415ms/step - loss: 2.1426 - acc: 0.5970 - val_loss: 3.8818 - val_acc: 0.3582\n",
            "Epoch 47/60\n",
            "781/781 [==============================] - 327s 418ms/step - loss: 2.2074 - acc: 0.5902 - val_loss: 3.2209 - val_acc: 0.4321\n",
            "Epoch 48/60\n",
            "781/781 [==============================] - 326s 418ms/step - loss: 1.8947 - acc: 0.6683 - val_loss: 2.9476 - val_acc: 0.4861\n",
            "Epoch 49/60\n",
            "781/781 [==============================] - 325s 417ms/step - loss: 1.9314 - acc: 0.6581 - val_loss: 3.5651 - val_acc: 0.3942\n",
            "Epoch 50/60\n",
            "781/781 [==============================] - 326s 417ms/step - loss: 2.5808 - acc: 0.5202 - val_loss: 4.6442 - val_acc: 0.2814\n",
            "\n",
            "Epoch 00050: saving model to /training2/cp-0050.ckpt\n",
            "Epoch 51/60\n",
            "781/781 [==============================] - 328s 420ms/step - loss: 2.6624 - acc: 0.5188 - val_loss: 3.3368 - val_acc: 0.4227\n",
            "Epoch 52/60\n",
            "781/781 [==============================] - 326s 417ms/step - loss: 2.2100 - acc: 0.6220 - val_loss: 2.9276 - val_acc: 0.4956\n",
            "Epoch 53/60\n",
            "781/781 [==============================] - 327s 419ms/step - loss: 1.9514 - acc: 0.6809 - val_loss: 3.1865 - val_acc: 0.4603\n",
            "Epoch 54/60\n",
            "781/781 [==============================] - 328s 420ms/step - loss: 2.1797 - acc: 0.6171 - val_loss: 3.4373 - val_acc: 0.4133\n",
            "Epoch 55/60\n",
            "781/781 [==============================] - 329s 421ms/step - loss: 2.2250 - acc: 0.6121 - val_loss: 3.2110 - val_acc: 0.4569\n",
            "\n",
            "Epoch 00055: saving model to /training2/cp-0055.ckpt\n",
            "Epoch 56/60\n",
            "781/781 [==============================] - 328s 420ms/step - loss: 1.9063 - acc: 0.6910 - val_loss: 2.9489 - val_acc: 0.5072\n",
            "Epoch 57/60\n",
            "781/781 [==============================] - 328s 420ms/step - loss: 1.6665 - acc: 0.7506 - val_loss: 2.9447 - val_acc: 0.5019\n",
            "Epoch 58/60\n",
            "781/781 [==============================] - 327s 419ms/step - loss: 1.5935 - acc: 0.7672 - val_loss: 2.9657 - val_acc: 0.5071\n",
            "Epoch 59/60\n",
            "781/781 [==============================] - 329s 421ms/step - loss: 1.5360 - acc: 0.7777 - val_loss: 3.0058 - val_acc: 0.4987\n",
            "Epoch 60/60\n",
            "781/781 [==============================] - 327s 418ms/step - loss: 1.4821 - acc: 0.7880 - val_loss: 3.0677 - val_acc: 0.4932\n",
            "\n",
            "Epoch 00060: saving model to /training2/cp-0060.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fecc88c7be0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    }
  ]
}